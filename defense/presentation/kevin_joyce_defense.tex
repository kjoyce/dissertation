% $Header: /home/vedranm/bitbucket/beamer/solutions/conference-talks/conference-ornate-20min.en.tex,v 90e850259b8b 2007/01/28 20:48:30 tantau $ %\documentclass[draft]{beamer}
\pdfminorversion=4
\documentclass[]{beamer}
%\usepackage{cmacros}


\usepackage{tikz}
\usepackage{cancel}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{arrows}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{fadings}
\usetikzlibrary{calc}
\usepackage{units}
\usepackage{listings}
\usepackage{multimedia}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amssymb}
% This breaks some things
\usepackage{mathrsfs}
\tikzset{
  mynodes/.style={text width=1.5cm}
}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\lstset{%
language=Python,
frame=single,
basicstyle=\footnotesize,
keywordstyle=\color{blue},
commentstyle=\color{dkgreen}
}
\definecolor{dkred}{rgb}{0.7,0,0}
\setbeamercolor{alerted text}{fg=dkred}

\newcommand{\eps}{\varepsilon}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%% Dissertation Header %%%%%%%%%%%%%%%%%%
\newcommand{\Z}{\ensuremath{\mathbb Z}}  % Integers 
\newcommand{\R}{\ensuremath{\mathbb R}}  % Real numbers
\newcommand{\RR}{\ensuremath{\mathbb R}}  % Deprecated reals
\newcommand{\C}{\ensuremath{\mathbb C}}  % Complex numbers
\renewcommand{\C}{\ensuremath{\mathbb C}}  % Complex numbers
\newcommand{\HH}{\ensuremath{\mathscr H}} % Hilbert space
\newcommand{\KK}{\ensuremath{\mathscr K}}
\newcommand{\pp}{\ensuremath{\mathscr P}}
\newcommand{\PP}{\ensuremath{\mathbb P}}
\newcommand{\DD}{\ensuremath{\mathscr D}}

\newcommand{\ds}{\displaystyle}

%% Operators
\newcommand{\G}{\ensuremath{\mathcal G}}
\newcommand{\F}{\ensuremath{\mathcal F}}
\newcommand{\B}{\ensuremath{\mathcal B}}
%\renewcommand{\G}{\ensuremath{\mathcal G}}
\newcommand{\K}{\ensuremath{\mathcal K}}
\newcommand{\LL}{\ensuremath{\mathcal L}}

\newcommand{\Ab}{{\bf A}}
\newcommand{\N}{\mathcal{N}}
%
%% Decorations
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\eqdef}{\stackrel{\rm def}{=}}
%
%% Shortened Symbols
%\newcommand{\eps}{\varepsilon}
\newcommand{\del}{\partial}
%
%% Vector Stuff
\newcommand{\bm}[1]{\mbox{\boldmath$#1$}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\rem}[2]{\bm{#1}_{\,\hat{#2}}}
%%%%%%%%%%%%% END Dissertation Header %%%%%%%%%%%%%%%%%%
% This file is a solution template for:

% - Talk at a conference/colloquium.
% - Talk length is about 20min.
% - Style is ornate.

% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{default}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)

%  \usecolortheme{seahorse}
%  \usecolortheme{rose}
%  \usefonttheme[onlylarge]{structuresmallcapsserif}
%  \usefonttheme[onlysmall]{structurebold}
%  \setbeamerfont{title}{shape=\itshape,family=\rmfamily}
%  \setbeamercolor{title}{fg=red!80!black}
}


\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}

\title{Point Spread Function Estimation and Uncertainty Quantification}
\author{Kevin Joyce$^{\dagger*}$\\ advised by John Bardsley$^\dagger$ and Aaron Luttman$^{*}$} %, \texttt{kevin1.joyce@umt.edu}}
\date[UM Colloquium]{May 5, 2016}

\institute[The University of Montana] % (optional, but mostly needed)
{
  $^\dagger$The University of Montana\\
  $^{*}$National Security Technologies LLC
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

%\date[ISPN '80] % (optional, should be abbreviation of conference name)
%{27th International Sumposium of Prime Numbers}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online


% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection]
%    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

%\beamerdefaultoverlayspecification{<+->}
\begin{document}
\nocite{bardsley2012mcmc,agapiou2014analysis,stuart2010,calvetti2007introduction,van2008partially}
\pgfdeclareimage[width=\paperwidth]{titlebackground}{figures/TitleSlide.pdf}
\setbeamertemplate{title page}{
\begin{picture}(0,0)
  \put(-30,-155){%
    \pgfuseimage{titlebackground}
  }
  \put(275,100){\scalebox{.5}{DOE/NV/25946-{}-\alert{????}}}
  \put(0,-110.7){%
    \begin{minipage}[b][15em][t]{\textwidth}
      \centering
      \begin{beamercolorbox}[sep=8pt,center]{title}
	\usebeamerfont{title}\inserttitle\par%
%	\ifx\insertsubtitle\@empty%
%	\else%
%	  \vskip0.25em%
%	  {\usebeamerfont{subtitle}\usebeamercolor[fg]{subtitle}\insertsubtitle\par}%
%	\fi%     
      \end{beamercolorbox}%
      \vskip1em\par
      \begin{beamercolorbox}[sep=8pt,center]{author}
	\usebeamerfont{author}\insertauthor
      \end{beamercolorbox}
      \begin{beamercolorbox}[sep=8pt,center]{institute}
	\usebeamerfont{institute}\insertinstitute
      \end{beamercolorbox}
      \begin{beamercolorbox}[sep=8pt,center]{date}
	\usebeamerfont{date}\insertdate
      \end{beamercolorbox}
    \end{minipage}
  }
\end{picture}
}
\setbeamertemplate{navigation symbols}{}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment to add outline of sections
\begin{frame}{Outline}
  \tableofcontents
  %\tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

% Structuring a talk is a difficult task and the following structure
% may not be suitable. Here are some rules that apply for this
% solution: 

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.

% - A conference audience is likely to know very little of what you
%   are going to talk about. So *simplify*!
% - In a 20min talk, getting the main ideas across is hard
%   enough. Leave out details, even if it means being less precise than
%   you think necessary.
% - If you omit details that are vital to the proof/implementation,
%   just say so once. Everybody will be happy with that.

%% Cygnus pictures
%\begin{frame}
%  \frametitle{Cygnus Radiographic Imaging System}
%  \begin{center}
%  \includegraphics[width=.8\textwidth]{figures/cygnus_pulse_side.pdf}
%  \end{center}
%\end{frame}
%

\section{Modeling Imaging Systems}
\subsection{Convolution with a point spread function}
%\begin{frame}
%  \frametitle{Imaging Model}
%\begin{tikzpicture}[scale=.8,every node/.style={minimum size=1cm},on grid] 
%  \def\myxslant{0.1}
%  \def\myyslant{-0.4}
%
%    \begin{scope}[
%            xshift=50,
%            every node/.append style={
%            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
%    ]
%	\node[shape = rectangle, anchor=south west, draw] (left) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand.png}};
%    \end{scope}
%
%    \begin{scope}[
%	    yshift=-40,
%    ] 
%%      \node[anchor=south west] (fruit) at (0,0){\includegraphics[width=20mm]{figures/hand.pdf}};
%    \end{scope}
%
%    \begin{scope}[
%            xshift=180,
%            every node/.append style={
%            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
%    ]
%	\node[shape = rectangle, anchor=south west, draw] (mid) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand_blur.pdf}};
%    \end{scope}
%
%    \begin{scope}[
%            xshift=300,
%            every node/.append style={
%            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
%            ]
%        %\fill[white,fill opacity=0.6] (0,0) rectangle (5,5);
%	\node[anchor=south west] (right) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand_blur.pdf}};
%        \draw[step=1mm, black] (0,0) grid (2.8,2.1); %defining grids
%    \end{scope}
%    	
%%    %putting arrows and labels:
%    \node at (2,2.5) (label1) {Ideal Image};
%    \alert{\draw[-latex,thick] (2,-1.5) to node[below] {Image filter}  (7.5,-1.5);}
%%    \node at (4.65,-3) (math1) {$\displaystyle{\int \alert{k(x-s)}\cdot \,ds }$};
%
%    \node at (12,2.5) (label2) {Measured image};
%    \draw[-latex,thick] (9,-1.5) to node[below]{Measurement error} (13.2,-1.5);
%%    \node at (11,-3) (math1) {$+\vect \eps \sim N(\vect 0,\lambda^{-1} \vect I)$};
%
%    \draw[-latex,thick,bend right] (12,3) to node[below]{Inverse problem}(2,3);
%\end{tikzpicture}
%\end{frame}

%\begin{frame}[t]
%  \frametitle{Imaging Model Assumptions}
%  \begin{itemize}
%  \itemsep 1.2em
%    \item A general model for blur in imaging is as a \alert{linear filter} $b = \mathcal Af$, where $f:\R^2 \to \R,b:\R^2\to \R,a:\R^2\times\R^2 \to \R$ and $$ b(\vect x) = \iint a(\vect s;\vect x) f(\vect s)d\vect s. $$
%    \item When the blur is \alert{translation invariant}, the model reduces to integral convolution when there exists a $k:\vect \R^2 \to \R$ so that $$b(\vect x) = \iint k(\vect x - \vect s) f(\vect s)d\vect s$$
%    \item When blur is \alert{direction invariant} or \alert{isotropic}, this results in one more reduction with $p:[0,\infty) \to \R$ and $$b(\vect x) = \iint p(|\vect x - \vect s|) f(\vect s)d\vect s $$
%    \item Measurement error is modeled as \alert{Gaussian} white noise.
%  \end{itemize}
%\end{frame}

\begin{frame}
  \frametitle{Imaging Model Assumptions}
\begin{tikzpicture}[scale=.8,every node/.style={minimum size=1cm},on grid] 
  \def\myxslant{0.1}
  \def\myyslant{-0.4}

    \begin{scope}[
            xshift=50,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
	\node[shape = rectangle, anchor=south west, draw] (left) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand.png}};
    \end{scope}

    \begin{scope}[
	    yshift=-40,
    ] 
%      \node[anchor=south west] (fruit) at (0,0){\includegraphics[width=20mm]{figures/hand.pdf}};
    \end{scope}

    \begin{scope}[
            xshift=180,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
	\node[shape = rectangle, anchor=south west, draw] (mid) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand_blur.pdf}};
    \end{scope}

    \begin{scope}[
            xshift=300,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
            ]
        %\fill[white,fill opacity=0.6] (0,0) rectangle (5,5);
	\node[anchor=south west] (right) at (0,0){\includegraphics[width=20mm]{figures/roentgen_hand_blur.pdf}};
        \draw[step=1mm, black] (0,0) grid (2.8,2.1); %defining grids
    \end{scope}
    	
%    %putting arrows and labels:
    \node at (2,2.5) (label1) {Ideal Image};
    \draw[-latex,thick] (2,-1.5) to node[below] {Image filter}  (7.5,-1.5);
    \node at (4.65,-3) (math1) {$b(x) = \displaystyle{\iint \alert{p(|x-s|)}\cdot f(s)\,ds }$};

    \node at (12,2.5) (label2) {Measured image};
    \draw[-latex,thick] (9,-1.5) to node[below]{Measurement error} (13.2,-1.5);
    \node at (11,-3) (math1) {$+\vect \eps \sim$ white noise};

    \draw[-latex,thick,bend right] (12,3) to node[below]{Inverse problem}(2,3);
\end{tikzpicture}
\end{frame}

\subsection{Estimating the PSF with calibration images}
\begin{frame}
  \frametitle{Point Spread Function Estimation}
\begin{tikzpicture}[scale=.8,every node/.style={minimum size=1cm},on grid] 
  \def\myxslant{0.1}
  \def\myyslant{-0.4}

    \begin{scope}[
            yshift=-20,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
    \pgfmathsetmacro{\cubex}{1}
    \pgfmathsetmacro{\cubey}{1}
    \pgfmathsetmacro{\cubez}{2}
    \draw[red,thick] (\cubex/2,\cubey/2,0) -- ++(3,3,3);
    \draw[gray,fill=black] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
    \draw[gray,fill=black] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
    \draw[gray,fill=black] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
    \end{scope}

    \begin{scope}[
            xshift=40,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
    \draw (0,0) rectangle (2.8,2.2);
    \end{scope}

    \begin{scope}[
            xshift=180,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
	\draw (0,0) rectangle (2.8,2.2);
	\tikzfading[name=fade out,inner color = transparent!0,outer color = transparent!100]
	\draw[path fading=fade out, fill=red,red] (1,1.2) circle (.1);
    \end{scope}

    \begin{scope}[
            xshift=300,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
            ]
        %\fill[white,fill opacity=0.6] (0,0) rectangle (5,5);
	\draw[path fading=fade out, fill=red,red] (1,1.2) circle (.1);
        \draw[step=1mm, black] (0,0) grid (2.8,2.2); %defining grids
    \end{scope}
    	

%    %putting arrows and labels:
    \node at (2,2.5) (label1) {Point Source};
    \draw[-latex,thick] (2,-1.5) to node[below] {Image Filter} (7.5,-1.5);
    \node at (4.65,-3) (math1) {$\displaystyle{\iint \alert{p(|x-s|)} \delta(s)\,ds}\quad\quad\quad$};
    \node at (4.65,-4.1) (math1) {$\displaystyle{\iint \alert{p(|s|)} \delta(x-s)\,ds = \alert{p(|x|)}}$};

    \node at (7,2.5) (label3) {Impulse Response};

    \node at (12,2.5) (label2) {PSF estimate};
    \draw[-latex,thick] (9,-1.5) to node[below]{Measurement error} (13.2,-1.5);
    \node at (11,-3) (math1) {$+\vect \eps \sim $ white noise};

\end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Point Spread Function Estimation}
\begin{tikzpicture}[scale=.8,every node/.style={minimum size=1cm},on grid] 
  \def\myxslant{0.1}
  \def\myyslant{-0.4}

    \begin{scope}[
            xshift=40,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
    \draw (0,0) rectangle (2.8,2.2);
    \draw[fill=black] (0,0) rectangle (1,2.2);
    \end{scope}

    \begin{scope}[
            yshift=-20,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
    \draw[x=.314cm,y=.2cm,z=.2cm,thick,-latex,dkred] (0,0,0) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1)
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1);
    \draw[x=.314cm,y=.2cm,z=.2cm,thick,-latex,dkred] (-2,0,0) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1);
    \draw[x=.314cm,y=.2cm,z=.2cm,thick,-latex,dkred] (-4,0,0) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1);

    \pgfmathsetmacro{\cubex}{2}
    \pgfmathsetmacro{\cubey}{2.5}
    \pgfmathsetmacro{\cubez}{1}
    \draw[black,fill=gray,opacity=.75] (3.7,3,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
    \draw[black,fill=gray,opacity=.75] (3.7,3,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
    \draw[black,fill=gray,opacity=.75] (3.7,3,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;

    \draw[x=.314cm,y=.2cm,z=.2cm,thick,-latex,dkred] (4,0,0) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1);
    \draw[x=.314cm,y=.2cm,z=.2cm,thick,-latex,dkred] (2,0,0) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1) 
      sin ++(0,1,1) cos ++(0,-1,1) sin ++(0,-1,1) cos ++(0,1,1);
    \end{scope}

    \begin{scope}[
            xshift=180,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
    ]
	\draw (0,0) rectangle (2.8,2.2);
	\tikzfading[name=fade left,left color = transparent!0,right color = transparent!100]
	\draw[path fading=fade left,fading transform={rotate=-30},fill=black] (.9,0) rectangle (1,2.2);
	\draw[fill=black] (0,0) rectangle (.9,2.2);
        \draw[blue] (0,1) -- (2.8,1);
    \end{scope}

    \begin{scope}[
            xshift=300,
            every node/.append style={
            xslant=\myxslant,yslant=\myyslant},xslant=\myxslant,yslant=\myyslant
            ]
	\draw (0,0) rectangle (2.8,2.2);
	\tikzfading[name=fade left,left color = transparent!0,right color = transparent!100]
	\draw[path fading=fade left,fading transform={rotate=-30},fill=black] (.9,0) rectangle (1,2.2);
	\draw[fill=black] (0,0) rectangle (.9,2.2);
        \draw[step=1mm, black] (0,0) grid (2.8,2.2); %defining grids
        \draw[blue] (0,1.05) -- (2.8,1.05);
    \end{scope}
    	

%    %putting arrows and labels:
%    \node at (,1.75) (bremsstrahlung) {\alert{Bremsstrahlung}};
%    \node at (,1.25) (bremsstrahlung) {\alert{X-rays}};
    \node at (2,2.5) (label1) {Opaque vertical edge};
    \draw[-latex,thick] (2,-1.5) to node[below] {Image Filter} (7.5,-1.5);
    \node at (4.65,-3) (math1) {$\displaystyle{\iint \alert{p(|x-s|)} E(s)\,ds}\quad\quad\quad$};
    \node at (4.65,-4.1) (math1) {$\displaystyle{\iint \alert{p(|s|)} E(x-s)\,ds = \color{blue}{b(x)}}$};

    \node at (7,2.5) (label3) {Blurred edge};

    \node at (12,2.5) (label2) {Noisy Data};
    \draw[-latex,thick] (9,-1.5) to node[below]{Measurement error} (13.2,-1.5);
    \node at (11,-3) (math1) {$+\vect \eps \sim$ white noise};

\end{tikzpicture}
\end{frame}

%\begin{frame}[t]
%  \frametitle{A Synthetic Example}
%  A radially symmetric two-dimensional \alert{Gaussian PSF} has the form
%  \begin{equation*}
%    p(r) = (2\pi\sigma^2)^{-1} e^{\frac{-r^2}{2\sigma^2}} 
%  \end{equation*}
%  and it can be shown analytically that the analytic forward blur is an \alert{error function}
%  \begin{equation*}
%    [\mathcal G p] (x_i) = \frac 1{\sqrt{2\pi}\sigma} \int_{-\infty}^{x_i} e^{-\frac{s^2}{2\sigma^2}}\,ds
%  \end{equation*} 
%  \begin{tikzpicture}[pic/.style={inner sep=0pt,above right}]
%    \node[pic] at (-1,2.5) (left)     { \includegraphics[width=.4\textwidth]{figures/radial_gaussian_psf.pdf} };
%    \node[pic] at (5,2.5) (right)     { \includegraphics[width=.4\textwidth]{figures/radial_gaussian_edgeblur.pdf} };
%    \path[bend left,-latex] (3,6) edge node[above]{$\mathcal G$} (6,6); 
%  \end{tikzpicture}
%\end{frame}


\begin{frame}
  \frametitle{X-ray Edge Calibration Data}
  \includegraphics[width=.5\textwidth]{partially_collapsed_gibbs/cygnusImage.pdf}
  \includegraphics[width=.5\textwidth]{partially_collapsed_gibbs/cygnusLineout.pdf}

  {\small Radiographic data from the Cygnus Dual Beam Radiography Facility at the NNSS in North Las Vegas.}
\end{frame}

\begin{frame}[t]
  \frametitle{General Edge Blur Problem}
  \vspace{-1em}
  $$
    \textcolor{blue}{b(x,y)} = \iint_{\RR^2} \alert{k(s,t)}E(x-s)dtds + \eps_{x,y},\quad E(x) =\begin{cases} 0 &x<0\\ 1 &x\ge 0.\end{cases}
  $$
  \hspace{-2em}
    %\begin{tikzpicture}[show background grid,pic/.style={inner sep=0pt,above right}]
    \begin{tikzpicture}[pic/.style={inner sep=0pt,above right}]
      %\fill (0,0) circle (2pt);


      \node[pic] at (-1,2) (conv1)     { \includegraphics[width=.5\textwidth]{figures/2d_radial_convolution_blank1.pdf} };
      \node[pic] at (5,2.5) (conv2)    { \includegraphics[width=.5\textwidth]{figures/2d_radial_convolution_blank2.pdf} };
      \node[pic] at (5.2,0) (lineout)  { \includegraphics[width=.5\textwidth]{figures/typical_line_out.pdf} };

      \tikzfading[name=fade out,inner color = transparent!0,outer color = transparent!80]
      \draw[fill=red,path fading=fade out, draw=none, rotate around={30:(2.1,1.2)}] (2.1,1.2) ellipse (.5 and 1);
%      \draw[fill=red,path fading=fade out, draw=none, rotate around={-50:(1.1,1.2)}] (1.1,1.2) ellipse (.5 and 1);
      \draw[fill=red,path fading=fade out, draw=none] (3.4,1.2) circle (.5);


      \node at (0,4) (t) {\footnotesize $t$};
      \node at (1.4,3) (s) {\footnotesize $s$};
      \node at (1.6,6) (y) {\footnotesize $y$};

      \path[bend left,dotted,-latex] (5.7,.8) edge node[above]{??} (3.2,.8);
      \path[bend left,-latex] (3.2,5.8) edge node[above]{\footnotesize System Response} (6,5.8);

    \end{tikzpicture}
\end{frame}

\begin{frame}[t]
  \frametitle{Radially Symmetric PSF}
  \vspace{-.5em}
  {\small
  We distinguish the \alert{radial profile} from the kernel by \alert{$k(s,t) = p\left(\sqrt{s^2+t^2}\right)$}%, and that the edge is indicated at $x=0$ by $E$, then
  \begin{align*}
    b(x,y) 
     &= \int_{-\infty}^\infty\int_{-\infty}^\infty k(s,t)\alert{E(x-s)}dtds + \eps_{x,y}\\
%     &= \int_0^\infty p(r) \alert{\left(\int_0^{2\pi} E(x - r\cos \theta)d \theta \right)\,r dr} + \eps_{x,y}\\
     &= \int_0^\infty p(r) \cdot \alert{g(x,r)\,r dr} + \eps_{x,y}.
  \end{align*}

  \begin{center}
\begin{tikzpicture}[scale=.8]
  \draw (0,-2.2) -- (0,2.2);
  \draw (-3,0) -- (3,0);
  \draw[dotted] (-2.5,-2.2) node[below]{\footnotesize$x<-r$} -- (-2.5,2.2);
  \draw[dotted] (2.5,-2.2) node[below]{\footnotesize$x>r$} -- (2.5,2.2);
  \draw[dotted] (-1,-2.2) node[below right]{\footnotesize$|x|\le r$} -- (-1,2.2);

  \draw (0,0) circle[radius=2];
  \draw[very thick,red,domain=120:240] plot ({2*cos(\x)},{2*sin(\x)});
  \draw (0,0) -- (-1,{sqrt(3)});
  \draw (.3,0) node[above right]{$\theta$} arc [radius=.3,start angle=0,end angle=120];
\end{tikzpicture}
  \includegraphics[width=.5\textwidth]{figures/g_function.pdf}

  Observe that $g$ is symmetric about $x=0$.
  \end{center}
  }
\end{frame}


%\begin{frame}
%  \frametitle{Radial Profile Inverse Problem}
%  For the inverse problem
%  \begin{equation*}
%    b = \mathcal G p + \eps,
%  \end{equation*}
%  \begin{itemize}
%%    \item $\mathcal G$ is a compact Hilbert-Schmidt operator, hence its inverse is unbounded.
%    \item Hence, the discretized problem $\vect G \vect p = \vect b$ results in an ill-conditioned a matrix 
%    \item The \alert{SVD} of a matrix: $\vect G = U \Sigma V^*$, so the left-inverse is $\vect G^\dagger = V \Sigma^\dagger U^*$.
%  \end{itemize}
%  \hspace{-2.5em}
%  \includegraphics[width=.39\textwidth]{figures/g_singular_values.pdf}
%  \includegraphics[width=.39\textwidth]{figures/g_singular_vectors.pdf}
%  \includegraphics[width=.39\textwidth]{figures/g_least_squares.pdf}
%\end{frame}


\section{Radial Symmetry for Function Spaces}
\subsection{Sobolev Spaces}
\subsection{Variable Transformation and the Pullback Operator}
\begin{frame}[t]
  \frametitle{Distributions and Sobolev spaces}
  \begin{itemize}
    \itemsep 1.2em
  \item Let $\phi \in \DD(\Omega)$ denote the space of compactly supported smooth functions defined on an open set $\Omega \subseteq \R^N$, called \alert{test functions}.
  \item The space of continuous linear functionals, denoted $f \in \DD^*(\Omega)$, are the \alert{distributions} on $\Omega$, where action of $f$ on $\phi$ is expressed by \alert{$\langle f, \phi\rangle$}.
  \item For functions, the action of the linear functional is \alert{$\langle f,g\rangle = \int f g\,dx$}.
  \item Operations are expressed adjointly, e.g. differentiation is given by integration by parts \alert{$Df(\phi) \eqdef -\langle f, D\phi \rangle$}.
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Distributions and Sobolev spaces}
  \begin{itemize}
    \itemsep 1.2em
    \item We define the \alert{$L^2$ inner-product} for test functions as the sesquilinear form $(\cdot,\cdot)_{L^2(\Omega)}:\DD(\Omega)\times\DD(\Omega) \to \C$ by the Riemann integral
    $$
      (\phi,\psi)_{L^2(\Omega)} \eqdef \int_{\Omega} \phi(x)\bar{\psi(x)}\,dx,
    $$
    with a norm $\|\cdot \|_{L^2(\Omega)}$.
    \item We can construct $L^2$ from test functions with a completion argument. Idea: Equivalence classes of $L^2$ \alert{Cauchy sequences of test functions $(\phi_n)$} correspond to \alert{$L^2$ distributions}.
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Distributions and Sobolev spaces}
  \begin{itemize}
    \itemsep 1.2em
%    \item We define the \alert{$L^2$ inner-product} for test functions as the sesquilinear form $(\cdot,\cdot)_{L^2(\Omega)}:\DD(\Omega)\times\DD(\Omega) \to \C$ by the Riemann integral
%    $$
%      (\phi,\psi)_{L^2(\Omega)} \eqdef \int_{\Omega} \phi(x)\bar{\psi(x)}\,dx,
%    $$
%    with a norm $\|\cdot \|_{L^2(\Omega)}$.
%    \item We can construct $L^2$ from test functions with a completion argument. Idea: Equivalence classes of $L^2$ \alert{Cauchy sequences of test functions $(\phi_n)$} correspond to \alert{$L^2$ distributions}.
    \item A \alert{Sobolev space} of order $n$ over an open set $\Omega \subseteq \R^k$ is $\HH^n(\Omega) = \{f \in L^2(\Omega): \del^\alpha f \in L^2(\Omega)\text{ whenever }|\alpha|\le N\}$.
    \item They are endowed with the sum of semi-norms
\begin{equation}
  \alert{(f,g)_{\Omega,n}} = \sum_{0\le |\alpha| \le n} \left( \del^\alpha f, \del^\alpha g\right)_{L^2(\Omega)}.
\end{equation}
    \item Each of these form a sequence of linear subspaces $\HH^n(\Omega) \subset \HH^{n-1}(\Omega) \subset \dots \subset \HH^1(\Omega) \subset L^2(\Omega)$, however, the inclusion is strict and they are not closed with respect to the $L^2$ norm.
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Variable Transformation and the Pullback Operator}
  \begin{itemize}
    \itemsep 1.2em
    \item Idea: Extend the notion of \alert{$k(x,y) = p\left(\sqrt{x^2+y^2}\right) = T^\sharp p$} to distributions ``adjointly'' as was done for derivatives: $\alert{\langle T^\sharp p,  \phi \rangle \eqdef \langle  p,  T_\sharp\phi \rangle }$
    \item Use \alert{change of variables} so that one component is $r = \sqrt{x^2 + y^2}$, for topological reasons, this can only be done
      on a \alert{proper subset} of $\R^2$.
    \item Let $T_{ij}(x,y) = \Big(\sqrt{x^2 + y^2}, (-1)^jy\Big)$, 
\begin{tikzpicture}[scale=.8]
  \draw[dotted,thick] (0,-2.2) -- (0,2.2);
  \draw[dotted,thick] (-3,0) -- (3,0);

  \draw[-latex] (2,0) node[above right]{$T_{0,0}^{-1}(r,t)$} arc [radius=2,start angle=0,end angle=90];
  \draw[-latex](-2,0) node[above left]{$T_{1,0}^{-1}(r,t)$} arc [radius=2,start angle=180,end angle=90];
  \draw[-latex](-2,0) node[below left]{$T_{1,1}^{-1}(r,t)$} arc [radius=2,start angle=180,end angle=270];
  \draw[-latex](2,0) node[below right]{$T_{0,1}^{-1}(r,t)$} arc [radius=2,start angle=360,end angle=270];
  ;
\end{tikzpicture}
    
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Radial Symmetry for Sobolev Spaces}
  \begin{itemize}
    \itemsep 1.2em
    \item The pullback by $T$ on $\DD^*(\Omega_1)$ is a linear operator $T^\sharp:\DD^*(\Omega_1) \to \DD^*(\Omega_2)$ that is \alert{injective, continuous, and unique}. %in the sense that $\langle T^\dagger \rho ,\phi\rangle = \langle \rho \circ T,\phi\rangle$ for all $\phi \in \DD(\Omega_2)$ and $\rho \in \DD(\Omega_1)$ implies $T^\dagger = T^\sharp$ in the sense of evaluation on distributions.
    \item {\bf Definition} $k\in \KK^n \subset \HH^n(\Omega_2)$, the space of \alert{radially symmetric distributions}, if there exists a sequence $(\rho_m) \subset \DD(\Omega_1)$, so that $(T^\sharp \rho_m)$ is Cauchy with respect to $\|\cdot \|_{\HH^k(\Omega_2)}$ and
  $$
  \Big\langle k,\phi\Big\rangle_{\Omega_2} = \lim_{n\to\infty}\left\langle T^\sharp \rho_n, \phi\right\rangle_{\Omega_2} = \lim_{m\to\infty}\Big\langle \rho_m, T_\sharp \phi\Big\rangle_{\Omega_1},
  $$
    \item {\bf Definition} The space of \alert{radial profiles} corresponding to $\KK^n$ distributions is $\pp^n = \{p \in \DD^*(\Omega_1): T^\sharp p \in \KK^n\}$.
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Radial Symmetry for Sobolev Spaces}
  \begin{itemize}
    \itemsep 1.2em
    \item The map $T^\sharp$ induces the inner product 
    $$
    \alert{(\rho,\omega)_{T(\Omega_1)}} = \Big( S_{1/2}(\rho),S_{1/2}(\omega) \Big)_{L^2(\Omega_1)}  %\|p\|_{T(\Omega_1)}^2 = \lim_{n\to\infty} \|(S_{1/2}(\rho_n)\|^2_{L^2(\Omega_1)} = \lim_{n\to\infty} \|T^\sharp \rho_n \|^2_{L^2(\Omega_2)}.
    $$ 
    where $S(\omega)$ is the \alert{shift operator} defined by $S(\omega) = \omega(r)\cdot\alert{\left(2\pi r\right)^{1/2}}$.
    \item When $k$ is a function, the familiar radial transformation is given
    $$
      \iint |k|^2\,dxdy = \int |p|^2 \,\alert{2\pi r}dr.
    $$
    \item Moreover, if $\rho,\omega \in \DD(\Omega_1)$, then \alert{the squared norm of the Laplacian} is given by 
    $$
      (\nabla T^\sharp \rho, \nabla T^\sharp \omega)_{L^2(\Omega_2)} = (\del \rho,\del \omega)_{T(\Omega_1)}.
    $$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Radial $L^2$ and the Laplacian}
  {\footnotesize For the two representations $k = T^\sharp p$}
  \begin{equation*}
   b= \mathcal F k \quad \implies\quad  b =  \alert{\left(\mathcal F T^\sharp\right)} p = \alert{\mathcal G} p 
  \end{equation*}
  {\footnotesize
%  regularization and uncertainty quantification can be achieved by modeling the PSF as a random quantity with an appropriate prior.
  \begin{itemize}
    \itemsep 1.2em
    \item When $k$ and $p$ are smooth, real-valued functions, the Laplacian on $\RR^2$  with the $L^2$ inner product translates to
    \begin{align*}
       \left\|\nabla k\right\|_{L^2(\Omega_2)}^2 
        = (\del p,\del p)_{T(\Omega_1)} 
        &= \int_0^\infty \frac{d}{dr} p(r)\cdot \frac{d}{dr} p(r) rdr\\
        &= \int_0^\infty p \cdot \alert{\underbrace{\frac 1r \frac{d}{dr}r \frac{d}{dr}}_{\mathcal L} }p(r) rdr
    \end{align*}
%      with the inner product $\ds{(p,q)_{rad} = \int p(r)\cdot \bar q(r)\, \alert{r dr}}$.
    \item We solve the inverse problem on the \alert{radial profile p}, with regularization on the \alert{radially symmetric function k}.
  \end{itemize}
  }
\end{frame}
\subsection{Regularization and Discrete representation}
\begin{frame}
  \frametitle{Tikhonov Laplacian Regularization}
  {\footnotesize For the two representations }
  \begin{equation*}
    b = \mathcal G p + \epsilon\quad\text{ and }\quad b= \mathcal F k + \epsilon
  \end{equation*}
  {\footnotesize
%  regularization and uncertainty quantification can be achieved by modeling the PSF as a random quantity with an appropriate prior.
  \begin{itemize}
    \itemsep 1.2em
    \item Minimizing the second order Tikhonov-Laplacian functional subject to $k$ radially symmetric
    $$ \frac \lambda 2 \Big\|b - \G p\Big\|_{L^2}^2 + \frac \delta 2 \left\|\nabla^2 k\right\|_{L^2}^2  $$
    \item is equivalent to minimizing 
    $$ \frac \lambda 2 \Big\|b - \G p\Big\|_{L^2}^2 + \frac \delta 2 \alert{\Big\|\LL^2 p\Big\|_{rad}^2}  $$
  \end{itemize}
  }
\end{frame}
\begin{frame}
  \frametitle{The discrete problem}
  {\footnotesize 
  In order to carry out estimation on a computer, we discretize the integral operator using \alert{mid-point quadrature}}
  $$
    b = \mathcal G p + \epsilon\quad \implies \quad \vect b = \vect G \vect p + \vect\epsilon
  $$
  {\footnotesize  Further, we discretize the regularization operator $\LL$ using \alert{finite differencing} }
%  regularization and uncertainty quantification can be achieved by modeling the PSF as a random quantity with an appropriate prior.
    $$ 
      \ds{\|\LL p\|_{rad}^2 = \int \left[\frac1r \frac{d}{dr} \left(r \frac d{dr} \right)\right]^2 p(r) rdr}
      \implies 
      \vect L \vect p = \vect r^{-1/2} \odot \vect D \left(\vect r \odot \vect D\vect p\right) 
    $$
    {\footnotesize and \alert{midpoint quadrature} for the inner products
    \begin{align*}
      \frac \lambda {2} \Big\|b - \G p\Big\|_{L^2}^2 &\implies \alert{\underbrace{\frac \lambda {2m}}_{\ds{\lambda}}} \Big\|\vect b - \vect G \alert{\vect p}\Big\|_{\R^m}^2 \\ 
      \intertext{ and }
      \frac\delta2 \Big\|\nabla^2 k\|_{L^2} \implies \frac \delta {2} \Big\|\LL p\Big\|_{rad}^2  &\implies \alert{\underbrace{\frac \delta {2n}}_{\ds{\delta}}} \Big\|\vect L  \alert{\vect p}\Big\|_{\R^n}^2  
    \end{align*}
    }
\end{frame}

\section{Hierarchical Bayesian Model}

\subsection{The posterior density}
\begin{frame}[t]
  \frametitle{Hierarchical Model for PSF estimation}
  Let $\pi(\vect x) = \PP(X = x)$ denote the probability density. For $\lambda,\delta$ and $\vect p$ and
  $$\vect b = \vect G \vect p + \vect \epsilon$$
  assume
  \begin{itemize}
    \itemsep 1.2em
    \item The \alert{likelihood} $\alert{\pi(\vect b|\vect p,\lambda,\delta) = \pi(\vect b| \vect p,\lambda) \propto \lambda^{M/2}\exp\left(-\frac\lambda2 \|\vect b - \vect G\vect p\|^2\right)}$ since $\vect \epsilon$ is independent Gaussian noise.
    \item The \alert{prior} \alert{$\pi(\vect p| \delta,\lambda) = \pi(\vect p| \delta) \propto \delta^{N/2}\exp\left(-\frac \delta{2} \|\vect L \vect p\|^2\right)$} since $k \sim \N(0,\nabla^{-2}) \implies p\sim N(0,\LL^{-2})$ 
    \item The \alert{hyperpriors} \alert{$\pi(\lambda) \propto \exp\left(-10^{-4}\lambda\right)$} and \alert{$\pi(\delta) \propto \exp\left(-10^{-4} \delta\right)$} are independent ``unobjective'' Gamma distributions.
  \end{itemize}
\end{frame}
\newcommand{\pib}{\pi_{\mathbf b}}
\begin{frame}[t]
  \frametitle{Bayesian Posterior}
  With $\pi(\vect b|\vect p,\lambda,\delta),\pi(\vect p| \delta,\lambda),$ and $\pi(\lambda,\delta)$, use Bayes' ``Theorem'' to obtain
  {\small
  \begin{align*}
    \pib(\alert{\vect p,\lambda,\delta}) &\eqdef \pi(\alert{\vect p,\lambda,\delta}|\vect b) = \pi(\vect b,\vect p,\lambda,\delta)/\pi(\vect b) \\
      &\propto {\alert{\lambda}}^{M/2}\alert{{\delta}}^{N/2}\exp\left(
        -\frac {\alert{\lambda}}{2} 
        \Big\|\vect b - \vect G \alert{\vect p}\Big\|_{\R^m} - \frac {\alert{\delta}} 2\Big\|\vect L  \alert{\vect p}\Big\|_{\R^n} - 10^{-4}(\alert\lambda + \alert\delta)
        \right) 
  \end{align*}
  }
  \begin{itemize}
    \itemsep 1.2em
    \item This is not a ``common'' probability density, hence simulations from a computer are not readily available.
    \item Bayes' ``Theorem'' will allow simulations from the \alert{full conditionals} $\pib(\lambda|\delta,\vect p), \pib(\delta|\lambda,\vect p)$ and $\pib(\vect p|\lambda,\delta)$.
    \item Because each distribution is from the \alert{exponential family}, they form a \alert{conjugacy} such that the full conditionals are ``shifts'' of the priors.
  \end{itemize}
\end{frame}

\begin{frame}[t]
  \frametitle{Full conditional densities}
  \begin{itemize}
  \itemsep 1.2em
  \item The resulting expressions are
{\small
\begin{align*}
  \pi(\lambda| \vect b,\vect p,\delta) 
  &%= \frac{\pi(\vect p,\lambda,\delta|\vect b)}{\pi(\vect p,\delta|\vect b) } 
  \propto \lambda^{(2N+1)/2+\alpha-1}\exp\left(-\lambda\left(\frac{1}{2}\|\vect{Gx} - \vect b\|^2 - \beta\right)  \right), \nonumber\\
  \pi(\delta| \vect b,\vect p,\lambda) 
  &%= \frac{\pi(\vect p,\lambda,\delta|\vect b)}{\pi(\vect p,\lambda|\vect b) } 
  \propto \delta^{N/2+\alpha-1}\exp\left(-\delta\left(\frac{1}{2}\vect \langle \vect p,\vect L\vect p \rangle- \beta\right)  \right), \\
  \pi(\vect p|\vect b,\lambda,\delta) 
%    &= \frac{\pi(\vect p,\lambda,\delta|\vect b)}{\pi(\lambda,\delta|\vect b) } \nonumber\\
    &\propto \exp\left(-\frac 12\Big\langle (\vect p-\vect m_{\lambda,\delta}),\alert{\vect J_{\lambda,\delta}}(\vect p - \vect m_{\lambda,\delta})\Big\rangle\right)
\end{align*}
}
where 
$$
  \vect J_{\lambda,\delta} \eqdef (\lambda \vect G^T\vect G + \delta \vect L) \quad\text{ and }\quad \vect m_{\lambda,\delta} \eqdef \alert{\vect J_{\lambda,\delta}^{-1}}\lambda \vect G^T\vect b,
$$
\item The matrix solves required for sampling can be efficiently computed using a \alert{Cholesky decomposition $\vect R_{\lambda,\delta}^T\vect R_{\lambda,\delta} \eqdef \vect J_{\lambda,\delta}$ in $O(N^3)$ flops}.
 \end{itemize} 

\end{frame}

\begin{frame}[t]
  \frametitle{Gibbs sampling}
  The Gibbs sampler [Geman and Geman 1984]: 
  
Given $\left(\lambda^{k-1},\delta^{k-1},\vect p^{k-1}\right)$, simulate
\begin{center}
\begingroup
\addtolength{\jot}{1em}
\begin{flalign*}
\itemsep 1.2em
1.&\text{ Simulate }\lambda^{k+1}\sim \Gamma\left((2N+1)/2+\alpha,\frac{1}{2}\Vert\vect G\vect p^{k}-\vect b\Vert^2+\beta\right).&\\
2.&\text{ Simulate }\delta^{k+1}\sim \Gamma\left(N/2+\alpha,\frac{1}{2}\left\langle\vect p^{k},\vect L\vect p^{k}\right\rangle+\beta\right).&\\
3.&\text{ Compute }\alert{\vect R_{\lambda^{k+1},\delta^{k+1}},\vect m_{\lambda^{k+1},\delta^{k+1}}}, \\
  &\text{ and set }\vect p^{k+1} = \vect R_{\lambda^{k+1},\delta^{k+1}}^{-1}\vect z + \vect m_{\lambda^{k+1},\delta^{k+1}}\text{ where }\vect z\sim \N\left(\vect 0,\vect I_{N\times N}\right).&
\end{flalign*}
\endgroup
\end{center}

\end{frame}


\begin{frame}[t]
  \frametitle{Correlated $\delta$ chains}
  \vspace{-1.2em}
  \begin{center}
    \includegraphics[width=.8\textwidth]{figures/psf_hierarchical_chains_50.pdf}\\
    \includegraphics[width=.8\textwidth]{figures/psf_hierarchical_chains_1000.pdf}\\ 
    %\includegraphics[width=.5\textwidth]{figures/psf_hierarchical_autocor_50.pdf}  \includegraphics[width=.5\textwidth]{figures/psf_hierarchical_chains_50.pdf}\\
    %\includegraphics[width=.5\textwidth]{figures/psf_hierarchical_autocor_500.pdf} \includegraphics[width=.5\textwidth]{figures/psf_hierarchical_chains_500.pdf}\\ 
    %\includegraphics[width=.5\textwidth]{figures/psf_hierarchical_autocor_1000.pdf} \includegraphics[width=.5\textwidth]{figures/psf_hierarchical_chains_1000.pdf}\\ 
  \end{center}
\end{frame}

\subsection{Gibbs Sampling and Partial Collapse}
\begin{frame}[t]
  \frametitle{Literature on the Issue}
  {\small
  \begin{itemize}
    \itemsep 1.2em
    \item \,[Agapiou,Bardsley,Stuart,Papaspiliopoulos, 2014] explained this phenomena theoretically for a general class of Laplacian based Hierarchical samplers for inverse problems.
    \item The issue arises when the discretization of \alert{$\vect p$} closer approximates the continuum, the correlation in the \alert{$\delta$} component of the Markov Chain becomes more correlated.
    \item \,[VanDyke, Park 2008] provide a general method for removing the dependence of problematic components in the Gibbs sampler, called \alert{partial collapse}.
    \item The idea has been independently derived in many places, however, if done carelessly [VanDyke, Park 2008] showed that the resulting Markov chain is no longer \alert{invariant}, although invariance was not proved there.
  \end{itemize}
  }
\end{frame}

\begin{frame}[t]
  \frametitle{Marginalized Sampler}
{\small
Given $\left(\lambda^{k-1},\delta^{k-1},\vect p^{k-1},\alert{\tilde{\vect p}^{k-1}}\right)$, simulate
\begin{center}
\begin{algorithmic}[1]
  \STATE $\lambda^k \sim \pib(\lambda|\delta^{k-1},\vect p^{k-1})$
  \STATE \textcolor{blue}{$(\delta^{k},\alert{\tilde{\vect p}^k}) \sim \pib(\delta,\alert{\tilde{\vect p}}|\lambda^k)$}
  \STATE \textcolor{blue}{$\vect p^{k} \sim \pib(\vect p|\lambda^k,\delta^k)$}
\end{algorithmic}
\end{center}
The associated Markov Chain is invariant with respect to $\tilde \pib(\lambda,\delta,\vect p,\tilde{\vect p}) = \alert{\pib(\lambda,\delta,\vect p)\pib(\tilde{\vect p}| \lambda,\delta)}$:  
}
{\footnotesize
\begin{align*}
 [\K\tilde\pib] 
\pause   &= \iiiint  \textcolor{blue}{\pib(\vect p'|\lambda',\delta')\pib(\delta',\tilde{\vect p}'|\lambda')}\pib(\lambda',\vect p|\delta)\alert{\tilde \pib(\lambda,\delta,\vect p,\tilde{\vect p})}d\lambda d\delta d\vect p d\tilde{\vect p} \\ 
\pause   &= \textcolor{blue}{\pib(\vect p'|\lambda',\delta')} \textcolor{blue}{\pib(\delta',\tilde{\vect p}'|\lambda')}\iint {\pib(\lambda'|\delta,\vect p)}\underbrace{\int\alert{\int \tilde \pib(\lambda,\delta,\vect p,\tilde{\vect p}) d\tilde{\vect p}}d\lambda }_{\pib(\delta,\mathbf p)}d\delta d\vect p \\
\pause   &= \textcolor{blue}{\underbrace{\pib(\vect p'|\lambda',\delta')}_{\pib(\mathbf p',\delta',\lambda')/\pib(\delta',\lambda')} \pib(\delta',\tilde{\vect p}'|\lambda')}\underbrace{\iint \pib(\lambda',\delta,\vect p) d\delta d\vect p}_{\pib(\lambda')} \\
\pause   &= \pib(\vect p',\lambda',\delta') \frac{\pib( \tilde {\vect p}',\lambda',\delta')}{\pib(\lambda',\delta')} \\
\pause   &= \tilde \pib(\lambda',\delta',\vect p',\tilde{\vect p}')
\end{align*}
}
\end{frame}

\begin{frame}[t]
  \frametitle{Partially Collapsed Sampler}
{\small
Given $\left(\lambda^{k-1},\delta^{k-1},\vect p^{k-1}\right)$, simulate
\begin{center}
\begin{algorithmic}[1]
  \STATE $\lambda^k \sim \pib(\lambda|\delta^{k-1},\vect p^{k-1})$
  \STATE $\delta^{k} \sim \pib(\delta|\lambda^k)$ 
  \STATE $\vect p^{k} \sim \pib(\vect p|\lambda^k,\delta^k)$
\end{algorithmic}
\end{center}
The associated Markov Chain is invariant with respect to $\int \tilde \pib(\lambda,\delta,\vect p,\tilde{\vect p}) = \int \pib(\lambda,\delta,\vect p)\pib(\tilde{\vect p}| \lambda,\delta) = \pib(\lambda,\delta,\vect p)$.
}

\begin{itemize}
  \itemsep 1.2em
  \item The order of the chain matters in the previous arguments.
  \item \alert{Permuting} steps 2 and 3 results in a chain that is no longer \alert{invariant} with respect to $\pib$.
  \item \alert{Cyclically permuting} results in a different sampler as well, however, this does not practically effect the overall chain, only the first and last steps, e.g. $(\K_1\K_3\K_2)^N = \K_1\alert{(\K_3\K_2\K_1)^{N-1}}\K_2\K_3$
\end{itemize}
\end{frame}
\begin{frame}[t]
\frametitle{Marginalized Posterior Density}
{\small
In order to sample $\pib(\delta|\lambda)$, we \alert{complete the square} of the quadratic form in $\pib(\lambda,\delta,\vect p)$ and integrate out $\lambda$, this results in
  \begin{align*}
    \pib( \delta|\lambda) 
    &\propto\exp\left( (n/2)\delta -  \alert{\ln |\det \vect J_{\lambda,\delta}|}  - \alert{\frac \lambda 2 \langle\vect b, \vect H_{\lambda,\delta}\vect b \rangle}  -10^{-4}\delta \right),
  \end{align*}
  where \alert{$\vect J_{\lambda,\delta} = \lambda \vect G^T\vect G + \delta \vect L$ and $\vect H_{\lambda,\delta} = \vect I - \lambda \vect G\vect J_{\lambda,\delta}^{-1}\vect G^T$}.
\begin{itemize}
\itemsep 1.2em
  \item To draw from this density, we embed a \alert{Metropolis-Hastings} algorithm within the Gibbs sampler.
  \item It can be shown that the resulting MCMC algorithm remains invariant.
%  \item Both constants can be carried out using a Cholesky factorization \alert{$\vect R_{\lambda,\delta}^T \vect R_{\lambda,\delta} = J_{\lambda,\delta}$} in ($O(n^3)$) flops, and will be required for each Metropolis step. 
  \item {\bf Corrollary }
    Suppose $\K = \K_m\dots\K_1$ is the transition operator for the Gibbs sampler, and $\tilde \K_i$ given $\rem xi$ is an operator such that $\tilde \K_i \big[\pi(x_i|\rem xi)\big] = \pi(x_1'|\rem xi)$, then $\K_m\dots\alert{\tilde \K_i}\K_{i-1}\dots\K$ is invariant with respect to $\pi$.
\end{itemize}
}
\end{frame}

\begin{frame}[t]
  \frametitle{Metropolis-Hastings within Gibbs Sampling}
  {\footnotesize
  \begin{itemize}
    \itemsep 1.1em
    \item We use $n_{MH}$ \alert{Metropolis-Hastings} steps using a Gaussian proposal with variance $\gamma$.
    \item Due to numerical overflow issues, all computations are carried out on the log scale.
    \item The full implementation is:
  \end{itemize}
  \begin{algorithmic}[1]
  \STATE Let $\lambda_0$, $\delta_0$,$\vect p$ and $\gamma$ be given.
  %STATE   \item Draw $\vect z_k$ from  $N(0,\vect I)$, and compute $\vect p_k = \vect S_k^{-1}(\vect z_k + \vect {S_k^T}^{-1}\lambda_k \vect A^T \vect b)$.
  \STATE Draw $\lambda_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\|\vect{Ax_k} - \vect b\|^2 - \beta \Big)$.
  %\STATE Compute $\vect {J_k} = \vect{R_k}^T \vect{R_k}$. 
  \STATE Set $j = 1$, Compute \alert{$\vect R_0^T \vect R_0  = \vect J_{\lambda^k,\delta^{k-1}}$, then $\pi_0 = \log \pib(\delta^{k-1}|\lambda^k)$}.
  \FOR{$1<j<n_{MH}$} 
    \STATE Draw $\tilde \delta$ from $N(\tilde \delta_{j-1}, \gamma)$.
    \STATE Compute \alert{$\tilde{\vect R}^T\tilde{\vect R} = J_{\lambda^k,\tilde \delta}$, then $\pi_j = \log p(\tilde \delta_j|\vect f \lambda _k)$}.
    \STATE Set $\tilde \pi = \pi_j, \vect R_j = \tilde{\vect R}$ and $\delta_{k} = \tilde \delta_j$ with probability $\tilde \pi/\pi_j$
  \ENDFOR
  \STATE Draw $\vect p_k$ from $N\Big( \vect R_k^{-1}\lambda_k \vect A^T\vect b, \vect R_k^{-1} \Big)$.
  \STATE Set $k=k+1$ and return to 2.
  \end{algorithmic}
  }
\end{frame}

\subsection{Results}

\begin{frame}[t]
  \frametitle{Marginalized $\delta$ chains}
  \vspace{-1.2em}
  \begin{center}
    \includegraphics[width=.8\textwidth]{figures/psf_marginalized_chains_50.pdf}\\
    \includegraphics[width=.8\textwidth]{figures/psf_marginalized_chains_1000.pdf}\\ 
  \end{center}
\end{frame}

\begin{frame}[t]
  \frametitle{Integrated Autocorrelation}
  \begin{itemize}
  \itemsep 1.2em
  \item The variance of the chain-mean estimator for the mean of the invariant density $\pi$ is
  $$
    {\rm Var}(\bar X_N) = \alert{\frac{1}{N^2}\sum_{k=1}^N{\rm Var}(X^k)}+\frac{1}{N^2}\sum_{k\neq l}^N {\rm Cov}(X^l,X^k).
  $$
  \item When $(X_k)$ are assumed to be nearly \alert{identically distributed} via the ergodic theorem, then for large $N$, we can approximate
  $$
    {\rm Var}(\bar X_N) \approx \frac{\sigma^2}{N}\sum_{k=-\infty}^\infty \rho(k) \text{ where }  \rho(k) \eqdef \frac{\mathrm{Cov}(X^1, X^{|k|})}{\sigma^2}.
  $$
  \item The function $\rho$ is called the \alert{normalized auto-correlation function} and the parameter $\displaystyle{\tau_{\rm int}\eqdef \sum_{k=-\infty}^\infty \rho(k)}$ is the \alert{integrated auto-correlation time}.
  \end{itemize}
  
\end{frame}
\begin{frame}[t]
  \frametitle{Comparing algorithms}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{frame}
\begin{frame}[t]
  \frametitle{Results}
\makebox[\textwidth][c]{
  \includegraphics[width=.45\textwidth]{partially_collapsed_gibbs/syntheticPSFrecon.pdf}
  \includegraphics[width=.6\textwidth]{figures/marginalized_pdf_psf_recon.pdf}
}

\vspace{3em}

{\tiny
  \begin{tabular}{l|ccccccc}
    \hline
% Jan. 9 values initial delta = 1 and initial lambda = 1
    Algorithm       & $\hat{\lambda}_{\rm MCMC}$& $\hat{\delta}_{\rm MCMC}$  & $\lambda$-$p_{\rm Geweke}$&$\delta$-$p_{\rm Geweke}$& IACT & ESS    & \#Chol/ESS \\
     & $(\times 10^{4})$ & ($\times 10^{-8}$) & & \\
    \hline
   Gibbs &                 1.102 &                 6.132 &                   0.998 &                    0.850& 36.2 &  138.0 &      \alert{72.4} \\
PC Gibbs &                 1.102 &                 5.611 &                   0.992 &                    0.943&  7.9 &  633.0 &      \alert{31.6} \\
\hspace{.2in} $n_{mh}= 1$ & & & & & & & \\
PC Gibbs &                 1.102 &                 5.515 &                   0.999 &                    0.985&  1.3 & 3799.6 &      \alert{15.8} \\
\hspace{.2in} $n_{mh}= 5$ & & & & & & & \\
%     MTC &                 1.099 &                 5.419 &                   0.998 &                    0.934& 11.5 &  473.2 &      21.1 \\
    % Dec. 11 values initial delta = norm(B) and initial lambda = 1/var( b(1:10) )
%    Gibbs             &                   0.997 &                    0.850& 46.3 &  107.9 &      92.6 \\
%    PC Gibbs $m_h=1$  &                   0.999 &                    0.955&  5.2 &  954.6 &      21.0 \\
%    PC Gibbs $m_h=5$  &                   0.998 &                    0.981&  1.7 & 2981.8 &      20.1 \\
%    MTC               &                   0.990 &                    0.950& 11.3 &  443.3 &      22.6 \\
    \hline
  \end{tabular}
}

\end{frame}
\begin{frame}[t]
  \frametitle{Results}
\makebox[\textwidth][c]{
  %\includegraphics[width=.45\textwidth]{partially_collapsed_gibbs/syntheticPSFrecon.pdf}
  %\includegraphics[width=.6\textwidth]{figures/marginalized_pdf_psf_recon.pdf}
  \includegraphics[width=.45\textwidth]{partially_collapsed_gibbs/cygnusPSFrecon.pdf}
  \includegraphics[width=.6\textwidth ]{partially_collapsed_gibbs/discrepencyPSF.pdf}
}

\vspace{3em}

{\tiny
  \begin{tabular}{l|ccccccc}
    \hline
    Algorithm           & $\hat{\lambda}_{\rm MCMC}$& $\hat{\delta}_{\rm MCMC}$& $\lambda$-$p_{\rm Geweke}$&$\delta$-$p_{\rm Geweke}$& IACT & ESS    & \#Chol/ESS \\
     & $(\times 10^{4})$ & ($\times 10^{-10}$) & & \\
    \hline
                  Gibbs &                 9.146 &               1.245  &                     0.995 &                    0.964& 14.0 &  357.6 &      \alert{28.0} \\
              PC Gibbs  &                 9.167 &               1.191  &                     0.995 &                    0.998&  8.5 &  587.3 &      \alert{34.1} \\
\hspace{.2in}$n_{mh}=1$ & & & & & & & \\
               PC Gibbs &                 9.178 &               1.189  &                     0.994 &                    0.980&  1.5 & 3278.5 &      \alert{18.3} \\
\hspace{.2in} $n_{mh}=5$ & & & & & & & \\
		    MTC &                 9.090 &               1.200  &                     0.996 &                    0.969& 12.5 &  432.2 &      \alert{23.1} \\
    \hline
  \end{tabular}
}

\end{frame}

\begin{frame}[t]
  \frametitle{Summary and Future Work}
  {\small
  \begin{itemize}
  \itemsep 1.2em
    \item We introduced a novel \alert{Hierarchical Bayesian non-parametric model} for estimating \alert{translation invariant} and \alert{isotropic} image blur with and edge.
    \item We developed the \alert{Partially Collapsed Gibbs sampler} from the Gibbs sampler, and showed when partial collapse remained \alert{stationary}.
    \item We then implemented the algorithm on a synthetic example using \alert{Metropolis with Partially Collapsed Gibbs}, and showed that it improves the standard Gibbs sampler.
    \item \alert{Future:} Develop the model and algorithm completely in infinite dimensions.
    \item \alert{Future:} Adapt the strategies to other imaging models that incorporate \alert{radial geometry} such as Abel and Radon transforms.
  \end{itemize}
  }
\end{frame}

\begin{frame}
  \frametitle{References}
  \bibliographystyle{alpha}
  {\footnotesize
  \bibliography{edge_spread}
  }
\end{frame}

\end{document}
