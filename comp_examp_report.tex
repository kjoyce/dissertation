\documentclass{article}
%\usepackage{cmacros}
\newcommand{\RR}{{\mathbb R}}
\newcommand{\NN}{{\mathbb N}}
\newcommand{\ZZ}{{\mathbb Z}}
\newcommand{\QQ}{{\mathbb Q}}
\newcommand{\CC}{{\mathbb C}}
\renewcommand{\emph}{\textit}
\newcommand{\contains}{{ \,\supseteq\, }}
\newcommand{\containedin}{{ \,\subseteq\, }}
\newcommand{\nonempty}{{ \not= \varnothing}}
\newcommand{\isempty}{{ = \varnothing}} 
\newcommand{\vect}[1]{\boldsymbol{#1}}
\renewcommand{\tilde}{\widetilde}
\newcommand{\ds}{\displaystyle}

\usepackage{mathdots}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{enumerate}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{units}
\usepackage{caption}
\usepackage{listings}
\usepackage{pgfplots}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{placeins}
\usepackage{floatpag}
\usetikzlibrary{positioning}
\usetikzlibrary{graphs}

\DeclareMathOperator{\cov}{cov}
%\pagestyle{fancy}
%\fancyhead{}
%\fancyfoot{}
%\fancyfoot[R]{\thepage}

\title{Marginalized sampling for finite dimensional Gaussian linear inverse problems}
\author{Kevin Joyce}
\date{May 22, 2015}
\begin{document} 
\newcommand{\figref}[1]{\figurename~\ref{#1}}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}
\renewcommand{\SS}{\mathcal S}
\newcommand{\HH}{\mathscr H}
\newcommand{\mom}{\widetilde}
\newcommand{\mle}{\widehat \Uptheta}
\newcommand{\eps}{\varepsilon}
\newcommand{\todist}{\stackrel{D}\longrightarrow}
\newcommand{\toprob}{\stackrel{p}\longrightarrow}
\newcommand{\TTheta}{\overline{\underline \Theta} }
\newcommand{\del}{\partial}
\newcommand{\approxsim}{\overset{\cdotp}{\underset{\cdotp}{\sim}}}
\newcommand{\RSS}{\ensuremath{\mathrm{RSS}}}
\newcommand{\MSE}{\ensuremath{\mathrm{MSE}}}
\newcommand{\SE}{\ensuremath{\mathrm{SE}}}
\newcommand{\TSS}{\ensuremath{\mathrm{TSS}}}
\newcommand{\Var}{\ensuremath{\mathrm{Var}}}
\newcommand{\SSReg}{\ensuremath{\mathrm{SSReg}}}
\newcommand{\E}{\ensuremath{\mathrm{\bf E}\,}}

\renewcommand{\a}[1]{{\color{red} \it #1}}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\lstset{language=Matlab,%
    basicstyle=\ttfamily\small,
    frame=single,%
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    %numbers=left,%
    %numberstyle={\tiny \color{black}},% size of the numbers
    %numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
} { }

\maketitle
\section{ Hierarchical Gibbs sampler }
The first section of this report details the implementation of the hierarchical Gibbs sampler with conjugate priors for one-dimensional deconvolution outlined in \cite{bardsley2012mcmc}.
The class of problems considered are linear inverse problems of the form
$$
  \vect b = \vect A \vect x + \vect \eta,
$$
where $\vect b\in \RR^m$ corresponds to observed data subject to random measurement error $\vect \eta \sim N(0,\lambda^{-1}\vect I)$; hence $\vect b|\vect x,\lambda \sim N(\vect{Ax},\lambda^{-1}\vect I)$ and the corresponding probability density, known as the likelihood, is given by
\begin{align*}
  p(\vect b | \vect x, \lambda) 
  &= \det(2\pi\lambda^{-1} \vect I)^{-1/2}\,\exp \left( -\frac 12 (\vect b - \vect{Ax})^T(\lambda^{-1}\vect{I})^{-1}(\vect b - \vect{Ax}) \right)\\
  &\propto \lambda^{n/2}\,\exp\left(-\frac \lambda2 \|\vect{Ax} - \vect b\|^2 \right). 
\end{align*}
The unknown $\vect x$ is transformed by an $m\times n$ matrix $\vect A$ that is a discretization of an operator on an infinite dimensional space and is badly conditioned in the sense that it has singular values clustered near zero.  
Furthermore, we take a Bayesian approach to modeling the quantity of interest, $\vect x$, as Gaussian process with zero mean and parametrically defined covariance $\delta \vect L^{-1}$. 
That is $\vect x|\delta \sim N(0,(\delta \vect L)^{-1})$, where $\vect L$ is a given positive definite matrix, with corresponding probability density 
\begin{align*}
  p(\vect x|\delta ) 
  &= \det(2\pi (\delta \vect L)^{-1})^{-1}\,\exp\left(-\frac 12 \vect x^T (\delta \vect L)\vect x\right) \\
  &\propto \delta^{n/2}\,\exp\left(-\frac \delta2 \vect x^T \vect L \vect x\right).
\end{align*}
Applying Bayes' theorem, we obtain the posterior density conditioned on $(\lambda,\delta)$,
\begin{align*}
  p(\vect x | \vect b, \lambda, \delta) 
  &= \frac{p(\vect b | \vect x,\lambda) p(\vect x | \delta)}{p(\vect b|\lambda,\delta)} %\\ 
%      \propto p(\vect b | \vect x,\lambda) p(\vect x | \delta) \\ 
  &\propto (\lambda\delta)^{n/2}\,\exp\left(-\frac{\lambda}{2}\|\vect{Ax} - \vect b\|^2 - \frac{\delta}{2}\vect x^T \vect L \vect x\right). 
\end{align*}
%    , where $Q$ is formed implicitly in terms of a matrix $L$ such that $Q = (L^TL)^{-1}$, hence $L\vect x \sim N(0,\delta^{-1}\vect I)$.  

Additionally, we assume the variables $\delta$ and $\lambda$ are assumed a priori to be independent Gamma random variables with ``uninformative'' hyper priors $\alpha_\lambda = \alpha_\delta = \alpha = 1$ and $\beta_\lambda = \beta_\delta = \beta =  10^{-4}$. 
%Hence, they each have probability densities
Hence, they have the joint density
\begin{align*}
%      p(\lambda) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\,\exp(-\beta\lambda),\quad\text{ and }\quad 
%      p(\delta) = \frac{\delta^{\alpha}}{\Gamma(\alpha)}\,\exp(-\beta\delta).
  p(\lambda,\delta) = p(\lambda)p(\delta) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\,\exp(-\beta\lambda) \frac{\delta^{\alpha}}{\Gamma(\alpha)}\,\exp(-\beta\delta).
\end{align*}
Applying Bayes' law then the definition of conditional probability, the posterior density is
\begin{align}
p(\vect x,\lambda,\delta|\vect b) 
&= \frac{p(\vect b|\vect x,\lambda,\delta) p(\vect x,\lambda,\delta)}{p(\vect b)} \nonumber \\
&= \frac{p(\vect b|\vect x,\lambda) p(\vect x| \delta) p(\lambda,\delta)}{p(\vect b)} \nonumber \\
&\propto \lambda^{n/2+\alpha}\delta^{n/2+\alpha} \exp\left(-\frac{\lambda}{2}\|\vect{Ax} - \vect b\|^2 - \frac{\delta}{2}\vect x^T \vect L \vect x - \beta\lambda - \beta\delta \right). \label{posterior_density}
\end{align}

Bayesian estimation and uncertainty quantification are accomplished by sampling from $p(\vect x,\lambda,\delta|\vect b)$ using Markov Chain Monte Carlo (MCMC) methods that use \eqref{posterior_density} to draw samples. In particular, a hierarchical Gibbs sampler can be used to construct a Markov chain whose equilibrium distribution is $p(\vect x,\lambda,\delta|\vect b)$ as in \cite{bardsley2012mcmc}. 
The hierarchical Gibbs sampler draws successively from the conditional densities of the joint posterior.  The conditional hyperparameter densities are
\begin{align*}
  p(\lambda| \vect b,\vect x,\delta) 
  &= \frac{p(\vect x,\lambda,\delta|\vect b)}{p(\vect x,\delta|\vect b) } 
  = \frac{p(\vect x,\lambda,\delta|\vect b)}{p(\vect x|\delta,\vect b)p(\delta) }
  \propto \lambda^{n/2+\alpha}\exp\left(-\lambda\left(\frac{1}{2}\|\vect{Ax} - \vect b\|^2 - \beta\right)  \right),\\
  p(\delta| \vect b,\vect x,\lambda) 
  &= \frac{p(\vect x,\lambda,\delta|\vect b)}{p(\vect x,\lambda|\vect b) } 
  = \frac{p(\vect x,\lambda,\delta|\vect b)}{p(\vect x|\vect b)p(\lambda) }
  \propto \delta^{n/2+\alpha}\exp\left(-\delta\left(\frac{1}{2}\vect x^T\vect L\vect x - \beta\right)  \right). 
\end{align*}
Observe that the conditional posterior distributions of $\lambda$ and $\delta$ are also Gamma distributed.
This notion is often referred to as conjugacy.

To sample from 
$$
  p(\vect x | \lambda,\delta,\vect b) \propto \exp\left(-\frac{\lambda}{2}\|\vect{Ax} - \vect b\|^2 - \frac{\delta}{2}\vect x^T \vect L \vect x\right),
$$
the following calculation isolates $\vect x$ into a quadratic form by completing the square and shows that the conditional density is proportional to a multivariate Gaussian density. 
Let $\vect R = (\lambda \vect A^T\vect A + \delta \vect L)$ a symmetric positive definite matrix (hence invertible), and $\vect \mu$ such that $\vect R\vect \mu = \lambda \vect A^T\vect b$, then observe
\begin{align}
  \lambda\|\vect{Ax} - \vect b\|^2 + \delta\vect x^T \vect L \vect x
  &= \vect x^T( \lambda \vect A^T \vect A + \delta \vect L)\vect x - 2 \lambda \vect x^T \vect A^T \vect b + \lambda \vect b^T \vect b\nonumber \\
  &= \vect x^T\vect R\vect x - 2 \vect x^T \vect R\vect \mu + \lambda \vect b^T \vect b\nonumber \\
  &= \vect x^T\vect R(\vect x - \vect \mu) - \vect x^T \vect R\vect \mu + \lambda \vect b^T \vect b\nonumber \\
  &= \vect (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) + \vect \mu^T\vect R\vect x - \vect \mu^T\vect R\vect \mu - \vect x^T \vect R\vect \mu + \lambda \vect b^T \vect b\nonumber \\
  &= \vect (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) - \vect \mu^T\vect R\vect \mu + \lambda \vect b^T \vect b. \label{factored_form}
\end{align}
Hence, 
$$
  p(\vect x|\vect b,\lambda,\delta) \propto \exp\left(-\frac12 (\vect x - \vect \mu)^T\vect R(\vect x - \vect \mu)\right),
$$ 
so $(\vect x|\vect b,\lambda,\delta) \sim N( \vect \mu, \vect R^{-1})$.

We remark that draws from this density dominate the computation time of the algorithm, since each involves a linear solve.  
For this problem, we compute a Cholesky decomposition $\vect {S_k}^T\vect{S_k} = \vect R_k = (\lambda_k \vect A^T\vect A + \delta_k L)$ ($O(n^3)$).
So now, given a random draw $\vect z_k \sim N(\vect 0, \vect I)$, 
$$
  \vect x_k := \vect S_k^{-1}(\vect z_k + \vect {S_k^{-1}}^T \lambda_k\vect A^T \vect b) = \vect S_k^{-1} \vect z_k + \vect {R_k}^{-1} \lambda_k\vect A^T \vect b \sim N(\vect {R_k}^{-1} \lambda_k\vect A^T \vect b,\vect {R_k}^{-1}). 
$$
The matrix $\vect S_k$ is lower triangular, so linear solves involving $\vect S_k$ and $\vect {S_k}^T$ are efficiently carried out using backward and forward substitution $(O(n^2))$, so the computationally expensive step is computing the factorization.  In the concluding remarks of the next section, we address alternatives to this factorization that are computationally less expensive.

The Gibbs sampler can now be defined:

\begin{algorithm}
\caption{Hierarchical Gibbs sampler}
\label{alg1}
\begin{algorithmic}[1]
  \STATE Draw $\delta_0$ and $\lambda_0$ from $\Gamma(\alpha,\beta)$ and set $k=0$.
  \STATE Compute $\vect {R_k} = \vect{S_k}^T \vect{S_k}$. 
  \STATE Draw $\vect x_k$ from $N\Big( \vect{R_k}^{-1}\lambda_k \vect A^T\vect b, \vect{R_k}^{-1} \Big)$.
  \STATE Draw $\lambda_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\|\vect{Ax_k} - \vect b\|^2 - \beta \Big)$
  \STATE Draw $\delta_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\vect x_k^T\vect L\vect x_k - \beta \Big)$
  \STATE Set $k=k+1$ and return to 2.
\end{algorithmic}
\end{algorithm}
%\textbf{Algorithm 1}
%\begin{enumerate} 
%  \setcounter{enumi}{-1}
%  \item Draw $\delta_0$ and $\lambda_0$ from $\Gamma(\alpha,\beta)$ and set $k=0$.
%  \item Draw $\vect x_k$ from $N\Big( (\lambda_k \vect A^T\vect A + \delta_k L)^{-1}\lambda_k \vect A^T\vect b, (\lambda_k \vect A^T\vect A + \delta_k \vect L)^{-1} \Big)$.
%  \item Draw $\lambda_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\|\vect{Ax_k} - \vect b\|^2 - \beta \Big)$
%  \item Draw $\delta_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\vect x_k^T\vect L\vect x_k - \beta \Big)$
%  \item Set $k=k+1$ and return to 1.
%\end{enumerate}

%Moreover, sampling from the distribution in step 1.~can be carried out by first drawing $\vect z_k \sim N(0,\vect I)$, then $\vect \mu_k + \vect L_k \vect z_k ~ N(\vect \mu_k, \vect L_k^T\vect L_k)$.
%  The algorithm is implemented in the \texttt{Matlab} file \texttt{non\_marginalized\_gibbs.m}.

This reconstruction and sampling scheme were implmented on the synthetically blurred signal used in \cite{bardsley2012mcmc} with one-dimensional convolution by a symmetric triangular kernel. 
The prior covariance, $\vect L$, is the discrete second order difference matrix with zero boundary conditions. 
In the left of \figref{naive_gibbs_recon}, we plot the signal and estimate (mean and 95\% percent credibility interval).  
Histograms for hyperparameters $\lambda$ and $\delta$ are given to the right. 
The first 100 ``burn-in'' samples were removed.

\begin{figure}
\makebox[\textwidth][c]{
  \includegraphics[width=.65\textwidth]{figures/problem1a1.pdf}
  \includegraphics[width=.65\textwidth]{figures/problem1a2.pdf}
}
  \caption{figure}{One dimensional numerical deblurring experiment. The left figure is the  MCMC mean reconstruction with 95\% credibility intervals  plotted over the true signal. 
    The upper figures are histograms of $\lambda_k$ and $\delta_k$ (with the first 100 burn-in samples removed). 
  }\label{naive_gibbs_recon}
\end{figure}

\begin{figure}
\makebox[\textwidth][c]{ \includegraphics[width=1.2\textwidth]{figures/problem1a3.pdf} }
  \caption{
    MCMC chains $\delta_k$ and $\lambda_k$ vs.~$k$ (with the first $100$ burn-in samples removed). 
  }\label{naive_gibbs_mcmc}
\end{figure}

In \figref{naive_gibbs_mcmc}, chain plots of the MCMC samples for $\lambda_k$ and $\delta_k$ versus the iterate number $k$ are given.  
Observe that the chain for $\delta$ shows evidence of auto-correlation with respect to lag in $k$. 
This effect is predicted by theory in \cite{agapiou2014analysis} as the number of discretization points increases. 
The effect of autocorrelation lowers the number of independent samples in the chain.  

In \figref{chain_deterioration}, we illustrate this by increasing discretization levels from $N=50$ to $N=1000$.  
Plots of the estimated autocorrelation as a function of lag are given to the left of the corresponding chain plot. 
Following \cite{sokal1989monte} \cite{green1992metropolis}, we also numerically assess the efficiency of the sampler by estimating the integrated autocorrelation time $ \tau = \sum_{k=-\infty}^\infty \rho_k$ where $\rho_k$ is the theoretical chain correlation,
We compute the effective sample size (ESS) by estimating $M / \hat\tau$ using Sokal's adaptive periodogram estimator for $\tau$ \cite{sokal1989monte}.  
The ESS is given in the titles of the autocorrelation plots.

\begin{figure}
\floatpagestyle{empty}
\vspace{-7em}
  $$ n = 50 $$
\makebox[\textwidth][c]{
  \includegraphics[width=.75\textwidth]{figures/problem1b50i.pdf}
  \includegraphics[width=.75\textwidth]{figures/problem1b50ii.pdf}
}
  $$ n = 100 $$
\makebox[\textwidth][c]{
  \includegraphics[width=.75\textwidth]{figures/problem1b100i.pdf}
  \includegraphics[width=.75\textwidth]{figures/problem1b100ii.pdf}
}
  $$ n = 500 $$
\makebox[\textwidth][c]{
  \includegraphics[width=.75\textwidth]{figures/problem1b500i.pdf}
  \includegraphics[width=.75\textwidth]{figures/problem1b500ii.pdf}
}
  $$ n = 1000 $$
\makebox[\textwidth][c]{
  \includegraphics[width=.75\textwidth]{figures/problem1b1000i.pdf}
  \includegraphics[width=.75\textwidth]{figures/problem1b1000ii.pdf}
}
  \caption{MCMC chains for $\lambda$ and $\delta$ after burn-in of 100 at various discretization levels are plotted on the left.  
    On the right, estimated autocorrelation as a function of lag are plotted.  
    Observe the quality of the samples for $\delta$ deteriorate as discretization becomes more refined. 
    The ESS estimate is given in the title of the autocorrelation plots.
  }\label{chain_deterioration}
\end{figure}

\section{ Metropolis-Hastings within hierarchical Gibbs sampler }
The authors of \cite{agapiou2014analysis} show that the deterioration of the $\delta$-chain results from hierarchical sampler's dependence on the discretization on $\vect x$.   
An alternative is to marginalize the sampling step for $\delta$, which integrates out $\vect x$-dependence from the posterior, yielding the probability density function
$$
  p(\lambda,\delta|\vect b) = \int_{\RR^n} p(\vect x,\lambda,\delta|\vect b)d\vect x.
$$
To compute $p(\lambda,\delta|\vect b)$ we continue the calculation in \eqref{factored_form} as follows
\begin{align*}
  \lambda\|\vect{Ax} - \vect b\|^2 + \delta\vect x^T \vect L \vect x
  &= (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) - \vect \mu^T\vect R\vect \mu + \lambda \vect b^T \vect b\\
  &= (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) - (\vect R\vect \mu)^T\vect R^{-1}(\vect R\vect \mu) + \lambda \vect b^T \vect b\\
  &= (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) - (\lambda \vect A^T \vect b)^T\vect R^{-1}(\lambda \vect A^T\vect b) + \lambda \vect b^T \vect b\\
  &= (\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu) + \vect b^T( \lambda\vect I -  \lambda^2 \vect A\vect R^{-1}\vect A^T)\vect b.\\
\end{align*}
  Define the quadratic form
\begin{equation}
  U(\lambda,\delta,\vect b) := \vect b^T( \lambda\vect I -  \lambda^2 \vect A\vect R^{-1}\vect A^T)\vect b, \label{exp_term_r}
\end{equation}
so that that the posterior can be expressed as
\begin{equation*}
  p(\vect x,\lambda,\delta|\vect b) \propto \lambda^{n/2+\alpha}\delta^{n/2+\alpha}  \exp\left( -\frac 12(\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu)\right) \exp\left(-\frac 12U(\lambda,\delta,\vect b)  -\beta\lambda - \beta \delta \right).
\end{equation*}
The first exponential term is the kernel to a multivariate Gaussian. We denote its normalizing constant as
\begin{align*}
  c(\lambda,\delta) 
  &= \int_{\RR^n} \exp\left(-\frac 12(\vect x-\vect \mu)^T\vect R(\vect x - \vect \mu)\right)d\vect x \\
  &= (2\pi)^{n/2} \det\left(\vect R^{-1}\right)^{1/2} \\
  &\propto \det(\lambda \vect A^T\vect A + \delta \vect L) ^{-1/2},
\end{align*}
and then integrate,
\begin{align}
  \int_{\RR^n} p(\vect x,\lambda,\delta|\vect b)d\vect x 
%  &\propto \lambda^{n/2+\alpha}\delta^{n/2+\alpha} \int_{\RR^n} \exp\left(-\frac{\lambda}{2}\|\vect{Ax} - \vect b\|^2 - \frac{\delta}{2}\vect x^T \vect L \vect x - \beta\lambda - \beta\delta \right) d\vect x \nonumber\\
  &\propto \lambda^{n/2+\alpha}\delta^{n/2+\alpha} c(\lambda,\delta) \exp\left(-\frac 12U(\lambda,\delta,\vect b) -\beta\lambda - \beta \delta \right). \label{marginal_pdf}
\end{align}
%where
%\begin{align*}
%  c(\lambda,\delta) &\propto \det(\lambda \vect A^T\vect A + \delta \vect L) ^{-1/2}, \\
%  U(\lambda,\delta,\vect b) 
%  &= \vect b^T( \lambda\vect I -  \lambda^2 \vect A\vect R^{-1}\vect A^T)\vect b \\
%  &= \vect b^T( \lambda\vect I -  \lambda^2 \vect A\vect (\lambda \vect A^T\vect A + \delta \vect L)^{-1}\vect A^T)\vect b.
%\end{align*}

Due to numerical issues related to overflow, we express each quantity above on the log scale for numerical implementation.
For small to medium scale problems where a Cholesky factorization $\vect R = \vect S^T \vect S$ is available (or a similar factorization where the determinant is easy to compute), we can express 
\begin{equation}
  \tilde c(\lambda,\delta) := \log(c(\lambda,\delta)^{-1/2}) = -\frac 12 \log \det\left(\vect S^T \vect S\right)  = -\frac 12 \log \prod_{i=1}^n s_{ii}^2  = - \sum_{i=1}^n \log s_{ii}. \label{det_term}
\end{equation}
Moreover, we can also use the Cholesky decomposition in \eqref{exp_term_r} so that computation of $U(\lambda,\delta,\vect b)$ can be efficiently carried out as
\begin{equation}
  U(\lambda,\delta,\vect b)  
= \lambda\vect b^T\vect b - \lambda^2 \vect b^T \vect A\vect S^{-1} ({\vect S^T}^{-1} \vect A^T\vect b). \label{exp_term}
\end{equation}
Finally, we can express the logarithm of the right hand side of \eqref{marginal_pdf} as
\begin{equation}
  P(U,\tilde c,\lambda,\delta) := (n/2+\alpha) \log \lambda\delta +  \tilde c(\lambda,\delta) - U(\lambda,\delta,\vect b) - \beta(\lambda + \delta).\label{mh_kernel}
\end{equation}
With \eqref{mh_kernel} in hand, we can employ a Metropolis-Hastings algorithm to sample from $p(\delta|\lambda,\vect b)$ within the Gibbs sampler defined in the previous section. 
Since the dependence of $\vect x$ was removed, and in particular its discretization level, we expect to obtain samples of $\delta$ without autocorrelation issues.

The Metropolis-Hastings step follows the outline provided in \cite{calvetti2007introduction} using a Gaussian proposal.
Formally, for a given proposal mean $\tilde \delta$ and variance $\gamma$, consider the white noise random walk proposal
$$
  q(\delta,\tilde \delta) = \frac {1}{\sqrt{2\pi\gamma}} \exp\left( -\frac 1{2\gamma} |\delta - \tilde \delta|^2\right).
$$
Since $q$ is a symmetric Gaussian, this leads to the simple acceptance ratio
$$
  \alpha(\delta,\tilde \delta) = \frac{p(\tilde \delta|\lambda,\vect b)}{p(\delta|\lambda,\vect b)}.
$$
That is, each Metropolis-Hastings step will accept $\tilde \delta$ with probability $\min\{1,\alpha(\delta,\tilde \delta)\}$.

To obtain the proposal parameters $\tilde \delta$ and $\gamma$, we run $N_1$ steps of the hierarchical Gibbs sampler and use the sample mean for the initial $\tilde \delta$ and the sample variance for $\gamma$.  
We then start a new Gibbs sampler with the embedded Metropolis-Hastings step.  

The Metropolis-Hastings sampler is embedded into the Gibbs sampler in Algorithm 1 with $\tilde M$ iterations.  
We use the current $\delta_k$ sample mean to initialize the Metropolis-Hastings sampler.
As the algorithm proceeds, $\gamma$ can be adapted to the current delta chain variance. 
As state before, the marginalized probabilities are computed on the log scale.
This algorithm is summarized in \textbf{Algorithm \ref{alg2}} and was implemented on the one-dimensional deconvolution problem from the previous section with $\tilde M = 10$ Metropolis-Hastings steps and $N_1 = 1000$ initial hierarchical samples to build the proposal parameters.
Results of the simulation are given in \figref{marginalized_recon}.

\begin{algorithm}[h]
\caption{Metropolis-Hastings within Gibbs sampler}
\label{alg2}
\begin{algorithmic}[1]
\STATE Run Algorithm 1 for $N_1$ steps to compute $\vect{\delta^\dagger} = (\delta_1^\dagger,\dots,\delta_{N_1}^\dagger)$ and compute $\gamma = \widehat{\Var\vect{\delta^\dagger}}$. Draw $\delta_0$ and $\lambda_0$ from $\Gamma(\alpha,\beta)$ and set $k=0$.
\STATE Compute $\vect {R_k} = \vect{S_k}^T \vect{S_k}$. 
\STATE Draw $\vect x_k$ from $N\Big( \vect R_k^{-1}\lambda_k \vect A^T\vect b, \vect R_k^{-1} \Big)$.
%STATE   \item Draw $\vect z_k$ from  $N(0,\vect I)$, and compute $\vect x_k = \vect S_k^{-1}(\vect z_k + \vect {S_k^T}^{-1}\lambda_k \vect A^T \vect b)$.
\STATE Draw $\lambda_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\|\vect{Ax_k} - \vect b\|^2 - \beta \Big)$.
\STATE Set $j = 0$, $\tilde \delta_0 = \frac 1k \sum_{i=1}^k \delta_i$.  Compute $c_0 = (\lambda_k,\tilde \delta_0)$ by \eqref{det_term}, $U_0 = U(\lambda_k,\tilde \delta_0,\vect b)$ by \eqref{exp_term}, then $p_0 = P(U_0,c_0,\lambda_k,\tilde \delta_0)$ by \eqref{mh_kernel}.
\WHILE{$j<\tilde M$} 
  \STATE Draw $\tilde \delta_j$ from $N(\tilde \delta_{j-1}, \gamma)$.
  \STATE Compute $U_j = U(\lambda_k,\tilde \delta_j)$, $c_j = c(\lambda_k,\tilde \delta_j)$, and $p_j=P(U_j,c_j,\lambda_k,\tilde \delta_j)$.
  \STATE Draw $u_j$ from $\mathrm{Unif}[0,1]$ and if $ p_j - p_{j-1} < \log u_j$ set $p_j = p_{j-1}$ and $\delta_{k} = \tilde \delta_j$. Otherwise, continue.
  \STATE Set $j=j+1$.
\ENDWHILE
\STATE Set $k=k+1$ and return to 2.
\end{algorithmic}
\end{algorithm}
%\textbf{Algorithm 2}
%\begin{enumerate} 
%\setcounter{enumi}{-1}
%\item Run Algorithm 1 for $N_1$ steps to compute $\vect{\delta^\dagger} = (\delta_1^\dagger,\dots,\delta_{N_1}^\dagger)$ and compute $\gamma = \widehat{\Var\vect{\delta^\dagger}}$. Draw $\delta_0$ and $\lambda_0$ from $\Gamma(\alpha,\beta)$ and set $k=0$.
%\item Draw $\vect x_k$ from $N\Big( \vect R_k^{-1}\lambda_k \vect A^T\vect b, \vect R_k^{-1} \Big)$.
%%	\item Draw $\vect z_k$ from  $N(0,\vect I)$, and compute $\vect x_k = \vect S_k^{-1}(\vect z_k + \vect {S_k^T}^{-1}\lambda_k \vect A^T \vect b)$.
%\item Draw $\lambda_{k+1}$ from $\Gamma \Big(n/2+\alpha,\,\frac12\|\vect{Ax_k} - \vect b\|^2 - \beta \Big)$.
%\item Set $j = 0$, $\tilde \delta_0 = \delta_k$.  \item Compute $U_0$ by \eqref{exp_term}, $c_0$ by \eqref{det_term}, and set $p =  (n/2+1)\log \lambda_k\delta_0 + c_0 - U_0 - \beta_k(\lambda_k+\delta_0)$.
%\item Do while $j < \tilde M$
%\begin{enumerate}[i.]
%  %\setcounter{enumii}{-1}
%  \item Draw $\tilde \delta_j$ from $N(\tilde \delta_{j-1}, \gamma)$.
%  \item Compute $U_j$ by \eqref{exp_term}, $c_j$ by \eqref{det_term}, and $p_j=  (n/2+1)\log \lambda_k\delta_j + c_j - U_j - \beta_k(\lambda_k+\delta_j)$.
%  \item Draw $u_j$ from $\mathrm{Unif}[0,1]$ and if $ p_j - p < \log u_j$ set $p = p_j$ and $\delta_{k} = \tilde \delta_j$. Otherwise set $\delta_k = \tilde \delta_{j-1}$.
%  \item Set $j=j+1$.
%\end{enumerate}
%\item Set $k=k+1$ and return to 1.
%\end{enumerate}

%Evaluation of the right hand side for a given linear inverse problem is implemented as \texttt{Matlab} function in the file \texttt{log\_marginalized\_pdf.m}, 
\begin{figure}[h]
\makebox[\textwidth][c]{
  \includegraphics[width=.7\textwidth]{figures/marginalized_recon.pdf}
  \includegraphics[width=.7\textwidth]{figures/marginalized_density.pdf}
}

\makebox[\textwidth][c]{
  \includegraphics[width=.7\textwidth]{figures/marginalized_auto.pdf}
  \includegraphics[width=.7\textwidth]{figures/marginalized_chains.pdf}
}
  \caption{In the upper left, the reconstruction is plotted with 95\% credibility bands over the true signal.  
    The upper right figure is the joint marginalized likelihood in \eqref{marginal_pdf} over the MCMC samples from Algorithm 2. 
    The lower left plots the autocorrelation in both $\lambda$ and $\delta$ with the ESS given in the title.  
    The lower right plots the MCMC chains for $\lambda$ and $\delta$.
  }\label{marginalized_recon}
\end{figure}

The algorithm was implemented at the $n=1000$ discretization level, and note that there is no evidence of autocorrelation in $\delta_k$ as was dramatically present in the output of the hierarchical Gibbs sampler. 
We remark that the computation time for \textbf{Algorithm \ref{alg2}} is increased dramatically as a Cholesky factorization is required for each inner loop of the sampler.  
The \texttt{MATLAB} implementation took approximately 30 minutes with an Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz core. 
However, this is still a dramatic decrease from the computational effort that would be required to obtain an equivalent number of independent samples from the hierarchical Gibbs sampler, which based on the estimate for integrated autocorrelation would be approximately 300 minutes ($\hat\tau$ times about 5 minutes).

The marginalization technique was specifically employed to address problems for fine levels of discretization, so methods that speed up the computation are natural next steps for investigation.
Since both $U(\lambda,\delta)$ and $c(\lambda,\delta)$ are scalar quantities only used in the acceptance step, they could be computed on a sparser grid  off-line and interpolated.  
More efficient methods for computing $U(\lambda,\delta)$ and $c(\lambda,\delta)$ that don't rely on Cholesky factorization $(O(n^3)$, such as Fourier methods implemented with the fast Fourier transform (FFT) algorithm ($O(n\log n)$) could also be investigated if the blurring operator is expressed as convolution.

\FloatBarrier
  \bibliographystyle{alpha}
  \bibliography{marginalized_sampling}
\end{document} 


